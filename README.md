# Awesome Visual Navigation

A curated list of research papers in Visual Navigation (VN). Link to the code and website if available is also present. You can also find more embodied vision papers in **[ awesome-embodied-vision](https://github.com/ChanganVR/awesome-embodied-vision)**.

## Contributing

Please feel free to contact me via email (sq008@mail.ustc.edu.cn) or open an issue or submit a pull request.

To add a new paper via pull request:

1. Fork the repo, edit `README.md`.

2. Put the new paper at the correct chronological position as the following format: <br>

   ```
   - **Paper Title** <br>
      *Author(s)* <br>
      Conference, Year. [[Paper]](link) [[Code]](link) [[Website]](link)
   ```

3. Send a pull request. Ideally, I will review the request within a week.

## Papers

### Tasks:

- **Target-driven Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments** <br>
   *Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko SÃ¼nderhauf, Ian Reid, Stephen Gould, Anton van den Hengel* <br>
   CVPR, 2018. [[Paper]](https://arxiv.org/abs/1711.07280) [[Code]](https://github.com/peteanderson80/Matterport3DSimulator) [[Website]](https://bringmeaspoon.org)


### Roadmap (Chronological Order):

- **Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation** <br>
   *Muhammad Zubair Irshad, Chih-Yao Ma, Zsolt Kira* <br>
   ICRA, 2021. [[Paper]](https://arxiv.org/abs/2104.10674) [[Code]](https://github.com/GT-RIPL/robo-vln) [[Website]](https://zubair-irshad.github.io/projects/robo-vln.html) [[Video]](https://www.youtube.com/watch?v=y16x9n_zP_4)
